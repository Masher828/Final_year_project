Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial 
intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human
intervention.Machine learning is the science of getting computers to act without being explicitly programmed. In the past
decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly
improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times
a day without knowing it.
Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, 
machine
learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved 
understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day 
without knowing it.

This model is based on the Naiye Bayes Theorem
It is a supervised learning machine learning process, which requires you to associate each dataset with a “sentiment” for
training.Sentiment analysis can be used to categorize text into a variety of sentiments

In this model we will build the naiye bayes classifier which can tell whether a given message have positive or negative 
sentiment . We can do this with the help of Baye's Theorem.

P(A|B)=P(B|A)P(A)/P(B)
where A and B are events P(B)≠0
-P(A) and P(B) are the probabilities of observing A and B without regard to each other.
-P(A|B) a conditional probability is the probability of observing event A given that B is true.
-P(B|A) is the probability of observing event B given that A is true.

A classifier based on the Naive Bayes algorithm.  In order to find the probability for a label, this algorithm first uses
the Bayes rule to express P(label|features) in terms of P(label) and P(features|label):

|                       P(label) * P(features|label)
|  P(label|features) = ------------------------------
|                              P(features)

The algorithm then makes the 'naive' assumption that all features are independent, given the label:

|                       P(label) * P(f1|label) * ... * P(fn|label)
|  P(label|features) = --------------------------------------------
|                                         P(features)

Rather than computing P(features) explicitly, the algorithm just calculates the numerator for each label, and normalizes
them so they sum to one:

|                       P(label) * P(f1|label) * ... * P(fn|label)
|  P(label|features) = --------------------------------------------
|                        SUM[l]( P(l) * P(f1|l) * ... * P(fn|l) )
